DxInfer is an element that performs AI model inference using the DEEPX NPU, processing input tensors from `DxPreprocess`.

- DxInfer receives preprocessed input tensors from `DxPreprocess` and performs inference using a specified AI model. 
- Each output tensor is assigned an ID specified by the `inference-id` property.
- The AI model to be used must be specified through the `model-path` property, pointing to a `.dxnn` file.

---

### **Key Features**

**Input Tensor Management**

- Input tensors are linked using the `preprocess-id` property.
- DXInfer performs inference on the input tensor generated by the desired DXPreprocess element, referencing the `preprocess-id`

**Output Tensor Management**

- Output tensors are linked using the `inference-id` property.
- Postprocess perform on the desired DXPostprocess element, referencing the `inference-id`

**Pipeline Configuration**

- Inference pipelines must follow the elements chain: `[DxPreprocess] -> [DxInfer] -> [DxPostprocess]`.
- Properties like `secondary-mode` must be consistent across `DxPreprocess`, `DxInfer` and `DxPostprocess` when processing the same AI model.

**QoS Handling**

- Input buffers may be dropped based on their timestamps when the sink element's `sync` property is `true`.  

**Throttle QoS Events**

- When `DxRate` sends a Throttle QoS Event, inference is delayed by the `throttling_delay` interval to prevent unnecessary NPU computations.
- This ensures smooth and consistent streaming by reducing the inference load for low framerate pipelines.

**Memory Management**

- Reuses preallocated memory blocks for output tensors, based on the `pool-size` property.

**Configuration via JSON**

- Properties can be set using a JSON configuration file specified through the `config-file-path` property.

---

### **Hierarchy**

```
GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBaseTransform
                         +----GstDxInfer
```

---

### **Properties**

| **Name**           | **Description**                                                                                      | **Type**  | **Default Value** |
|---------------------|------------------------------------------------------------------------------------------------------|-----------|--------------------|
| `name`             | Sets the unique name of the DxInfer element.                                                        | String    | `"dxinfer0"`       |
| `config-file-path` | Path to the JSON config file containing the element's properties.                                    | String    | `null`             |
| `model-path`       | Path to the `.dxnn` model file used for inference.                                                  | String    | `null`             |
| `preprocess-id`    | Specifies the ID of the input tensor to be used for **inference**.                                       | Integer   | `0`                |
| `inference-id`     | Specifies the ID of the output tensor to be used for **postprocess**.                                                       | Integer   | `0`                |
| `secondary-mode`   | Determines whether to operate in primary mode or secondary mode.                                     | Boolean   | `false`            |
| `pool-size`        | Specifies the number of preallocated memory blocks for output tensors.                               | Integer   | `1`                |

---

### **Example JSON Configuration**

```json
{
    "preprocess_id": 1,
    "inference_id": 1,
    "model_path" : "./dx_stream/samples/models/YOLOV5S_1.dxnn"
}
```

---

### **Notes**
- The pipeline must follow `[DxPreprocess] -> [DxInfer] -> [DxPostprocess]` for proper operation.
- Properties can also be configured using JSON for user convenience.



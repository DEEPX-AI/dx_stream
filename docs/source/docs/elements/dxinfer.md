DxInfer is an element that performs AI model inference using the DEEPX NPU, processing input tensors from `DxPreprocess`.

- DxInfer receives preprocessed input tensors from `DxPreprocess` and performs inference using a specified AI model. 
- The inference results (output tensors) are then forwarded to `custom library function` for post processing.
- The AI model to be used must be specified through the `model-path` property, pointing to a `.dxnn` file.

---

### **Key Features**

**Input Tensor Management**

- Input tensors are linked using the `preprocess-id` property.
- DXInfer performs inference on the input tensor generated by the desired DXPreprocess element, referencing the `preprocess-id`

**Pipeline Configuration**

- Inference pipelines must follow the structure: `[DxPreprocess] -> [DxInfer]`.
- Properties like `secondary-mode` must be consistent across `DxPreprocess` and `DxInfer` when processing the same AI model.

**QoS Handling**

- Input buffers may be dropped based on their timestamps when the sink element's `sync` property is `true`.  

**Throttle QoS Events**

- When `DxRate` sends a Throttle QoS Event, inference is delayed by the `throttling_delay` interval to prevent unnecessary NPU computations.
- This ensures smooth and consistent streaming by reducing the inference load for low framerate pipelines.

**Memory Management**

- Reuses preallocated memory blocks for output tensors, based on the `pool-size` property.

**Configuration via JSON**

- Properties can be set using a JSON configuration file specified through the `config-file-path` property.

---

### **Hierarchy**

```
GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBaseTransform
                         +----GstDxInfer
```

---

### **Properties**

| **Name**           | **Description**                                                                                      | **Type**  | **Default Value** |
|---------------------|------------------------------------------------------------------------------------------------------|-----------|--------------------|
| `name`             | Sets the unique name of the DxInfer element.                                                        | String    | `"dxinfer0"`       |
| `config-file-path` | Path to the JSON config file containing the element's properties.                                    | String    | `null`             |
| `model-path`       | Path to the `.dxnn` model file used for inference.                                                  | String    | `null`             |
| `preprocess-id`    | Specifies the ID of the input tensor to be used for inference.                                       | Integer   | `0`                |
| `library-file-path`     | Path to the custom postprocess library.                                                       | String   | `null`                |
| `function-name`             | Specifies which function in the custom postprocess library to use.                 | String   | `null`            |
| `secondary-mode`   | Determines whether to operate in primary mode or secondary mode.                                     | Boolean   | `false`            |
| `pool-size`        | Specifies the number of preallocated memory blocks for output tensors.                               | Integer   | `1`                |

---

### **Example JSON Configuration**

```json
{
    "preprocess_id": 1,
    "model_path" : "./dx_stream/samples/models/YOLOV5S_1.dxnn",
    "library_file_path": "/usr/share/dx-stream/lib/libpostprocess_yolo.so",
    "function_name": "YOLOV5S_1"
}
```

---

### **Notes**
- The pipeline must follow `[DxPreprocess] -> [DxInfer]` for proper operation.
- Properties can also be configured using JSON for user convenience.


